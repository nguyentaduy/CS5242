{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras import optimizers\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import Lambda, Activation,recurrent, Bidirectional, Dense, Flatten, Conv1D, Dropout, LSTM, GRU, concatenate, multiply, add, Reshape, MaxPooling1D, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dictionary(vocab):\n",
    "    d = dict()\n",
    "    with open(vocab) as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            values = l.strip().split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            d[word] = coefs\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = get_dictionary(\"glove/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "titles = dict()\n",
    "contexts = dict()\n",
    "questions = dict()\n",
    "answers_text = dict()\n",
    "answers_start = dict()\n",
    "for i in range(len(data)):\n",
    "    paragraphs = data[i][\"paragraphs\"]\n",
    "    title = data[i][\"title\"]\n",
    "    for j in range(len(paragraphs)):\n",
    "        context = paragraphs[j][\"context\"]\n",
    "        qas = paragraphs[j][\"qas\"]\n",
    "        for k in range(len(qas)):\n",
    "            id_ = qas[k][\"id\"]\n",
    "            answer = qas[k][\"answer\"]\n",
    "            question = qas[k][\"question\"]\n",
    "            ids.append(id_)\n",
    "            titles[id_] = title\n",
    "            contexts[id_] = context\n",
    "            answers_start[id_] = answer[\"answer_start\"]\n",
    "            answers_text[id_] = answer[\"text\"]\n",
    "            questions[id_] = question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_para = 600\n",
    "max_q = 50\n",
    "dimension = 300\n",
    "train_len = len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paras = np.zeros((train_len, max_para, dimension))\n",
    "qns = np.zeros((train_len, max_q, dimension))\n",
    "exact_match = np.zeros((train_len, max_para, 3))\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n"
     ]
    }
   ],
   "source": [
    "for i in range(train_len):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    words = word_tokenize(contexts[ids[i]])\n",
    "    qs = word_tokenize(questions[ids[i]])\n",
    "    qs_1 = [w.lower() for w in qs]\n",
    "    qs_2 = [lmtzr.lemmatize(w) for w in qs_1]\n",
    "    for j in range(min(max_para - 1, len(words))):\n",
    "        if words[j].lower() in d:\n",
    "            paras[i][j] = d[words[j].lower()]\n",
    "        if words[j] in qs:\n",
    "            exact_match[i][j][0] = 1\n",
    "        if words[j].lower() in qs_1:\n",
    "            exact_match[i][j][1] = 1\n",
    "        if lmtzr.lemmatize(words[j].lower()) in qs_2:\n",
    "            exact_match[i][j][2] = 1\n",
    "    for j in range(min(max_q - 1, len(qs))):\n",
    "        if qs[j].lower() in d:\n",
    "            qns[i][j] = d[qs[j].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_hot_answer(para, answer, answer_start, option, max_length):\n",
    "    if option == \"s\":\n",
    "        from_begin = para[0:answer_start]\n",
    "    else:\n",
    "        from_begin = para[0:answer_start+len(answer)]\n",
    "    l = len(word_tokenize(from_begin))\n",
    "    one_hot = np.zeros(max_length)\n",
    "    if option == \"s\":\n",
    "        one_hot[min(max_para-1,l)] = 1\n",
    "    else:\n",
    "        one_hot[min(max_para-1,l-1)] = 1\n",
    "    return one_hot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans_starts = [create_one_hot_answer(contexts[ids[i]], answers_text[ids[i]], answers_start[ids[i]], \"s\", max_para) for i in range(train_len)]\n",
    "ans_ends = [create_one_hot_answer(contexts[ids[i]], answers_text[ids[i]], answers_start[ids[i]], \"e\", max_para) for i in range(train_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans_s = np.array(ans_starts)\n",
    "ans_e = np.array(ans_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, name, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.name = name\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name=self.name, \n",
    "                                      shape=(input_shape[2], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.W)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = layers.Input(shape=(max_para,dimension), dtype='float32')\n",
    "Q = layers.Input(shape=(max_q,dimension), dtype='float32')\n",
    "P_exact_match = layers.Input(shape=(max_para,3), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = MyLayer(128,\"alpha\")\n",
    "beta = MyLayer(1,\"beta\")\n",
    "gamma_s = MyLayer(128, \"gamma_s\")\n",
    "gamma_e = MyLayer(128, \"gamma_e\")\n",
    "hidden_unit = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aligned_P = layers.Activation(\"relu\")(alpha(P))\n",
    "aligned_Q = layers.Activation(\"relu\")(alpha(Q))\n",
    "coatt_aligned_PQ = layers.dot([aligned_Q, aligned_P], axes=2)\n",
    "coatt_aligned_PQ = Reshape((max_para * max_q,))(coatt_aligned_PQ)\n",
    "coatt_aligned_PQ = layers.Activation(\"softmax\")(coatt_aligned_PQ)\n",
    "coatt_aligned_PQ = Reshape((max_q, max_para,))(coatt_aligned_PQ)\n",
    "coatt_aligned_PQ = layers.dot([coatt_aligned_PQ,Q],axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_P = layers.concatenate([P, P_exact_match, coatt_aligned_PQ], axis=2)\n",
    "final_P = Dropout(.3)(final_P)\n",
    "final_P = Bidirectional(LSTM(hidden_unit,return_sequences=True))(final_P)\n",
    "final_P = Dropout(.3)(final_P)\n",
    "final_P = Bidirectional(LSTM(hidden_unit,return_sequences=True))(final_P)\n",
    "final_P = Dropout(.3)(final_P)\n",
    "final_P = Bidirectional(LSTM(hidden_unit,return_sequences=True))(final_P)\n",
    "final_P = Dropout(.3)(final_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_Q = Dropout(.3)(Q)\n",
    "final_Q = Bidirectional(LSTM(hidden_unit,return_sequences=True))(final_Q)\n",
    "final_Q = Dropout(.3)(final_Q)\n",
    "final_Q = Bidirectional(LSTM(hidden_unit,return_sequences=True))(final_Q)\n",
    "final_Q = Dropout(.3)(final_Q)\n",
    "final_Q = Bidirectional(LSTM(hidden_unit,return_sequences=True))(final_Q)\n",
    "final_Q = Dropout(.3)(final_Q)\n",
    "w = beta(final_Q)\n",
    "final_Q = layers.dot([w,final_Q],axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = gamma_s(final_P)\n",
    "start = layers.dot([start, final_Q], axes=2)\n",
    "start = Reshape((max_para,))(start)\n",
    "start = layers.Activation(\"softmax\")(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end = gamma_e(final_P)\n",
    "end = layers.dot([end, final_Q], axes=2)\n",
    "end = Reshape((max_para,))(end)\n",
    "end = layers.Activation(\"softmax\")(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model([P, Q, P_exact_match],[start, end])\n",
    "model.load_weights('drqa.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adamax\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 600, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 50, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "my_layer_16 (MyLayer)           multiple             38400       input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 50, 128)      0           my_layer_16[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 600, 128)     0           my_layer_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_33 (Dot)                    (None, 50, 600)      0           activation_27[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 30000)        0           dot_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 30000)        0           reshape_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 50, 600)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 600, 3)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_34 (Dot)                    (None, 600, 300)     0           reshape_27[0][0]                 \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 600, 603)     0           input_16[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "                                                                 dot_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 50, 300)      0           input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 600, 603)     0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_58 (Bidirectional (None, 50, 128)      186880      dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_55 (Bidirectional (None, 600, 128)     342016      dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 50, 128)      0           bidirectional_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 600, 128)     0           bidirectional_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_59 (Bidirectional (None, 50, 128)      98816       dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_56 (Bidirectional (None, 600, 128)     98816       dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 50, 128)      0           bidirectional_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 600, 128)     0           bidirectional_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_60 (Bidirectional (None, 50, 128)      98816       dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_57 (Bidirectional (None, 600, 128)     98816       dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 50, 128)      0           bidirectional_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 600, 128)     0           bidirectional_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "my_layer_17 (MyLayer)           (None, 50, 1)        128         dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "my_layer_18 (MyLayer)           (None, 600, 128)     16384       dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dot_35 (Dot)                    (None, 1, 128)       0           my_layer_17[0][0]                \n",
      "                                                                 dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "my_layer_19 (MyLayer)           (None, 600, 128)     16384       dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dot_36 (Dot)                    (None, 600, 1)       0           my_layer_18[0][0]                \n",
      "                                                                 dot_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_37 (Dot)                    (None, 600, 1)       0           my_layer_19[0][0]                \n",
      "                                                                 dot_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 600)          0           dot_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 600)          0           dot_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 600)          0           reshape_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 600)          0           reshape_29[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 995,456\n",
      "Trainable params: 995,456\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "*********************-- 0 --*********************\n",
      "Train on 55241 samples, validate on 6138 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "print('Training')\n",
    "for i in range(10):\n",
    "    print(\"*********************--\", i, \"--*********************\")\n",
    "    model.fit([paras, qns, exact_match], [ans_s, ans_e],\n",
    "              batch_size=32, epochs=5, validation_split=0.1)\n",
    "    model.save('drqa.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
