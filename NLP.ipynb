{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import environ\n",
    "environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras\n",
    "from keras.layers import recurrent, Bidirectional, Dense, Flatten, Conv1D, Dropout, Embedding, GRU, LSTM, Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dictionary(vocab):\n",
    "    d = dict()\n",
    "    with open(vocab, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            w = l.strip().split()\n",
    "            k = w[0]\n",
    "            v = []\n",
    "            for i in range(1, len(w)):\n",
    "                v.append(float(w[i]))\n",
    "            d[k] = np.array(v, dtype=np.float32)\n",
    "    return d\n",
    "def tokenize_sequence(seq, max_length, dim):\n",
    "    words = word_tokenize(seq.lower())\n",
    "    word_vectors = list(map(lambda x: d[x] if x in d else np.zeros(dim), words))\n",
    "    for i in range(len(word_vectors), max_length):\n",
    "        word_vectors.append(np.zeros(dim))\n",
    "    return np.array(word_vectors)\n",
    "def create_one_hot_answer(para, answer, answer_start, option, max_length):\n",
    "    if option == \"s\":\n",
    "        from_begin = para[0:answer_start]\n",
    "    else:\n",
    "        from_begin = para[0:answer_start+len(answer)]\n",
    "    l = len(word_tokenize(from_begin))\n",
    "    one_hot = np.zeros(max_length)\n",
    "    if option == \"s\":\n",
    "        one_hot[l] = 1\n",
    "    else:\n",
    "        one_hot[l - 1] = 1\n",
    "    return one_hot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "ids = []\n",
    "titles = dict()\n",
    "contexts = list()\n",
    "questions = list()\n",
    "contexts_uni = dict()\n",
    "questions_uni = dict()\n",
    "answers_text = dict()\n",
    "answers_start = dict()\n",
    "for i in range(len(data)):\n",
    "    paragraphs = data[i][\"paragraphs\"]\n",
    "    title = data[i][\"title\"]\n",
    "    for j in range(len(paragraphs)):\n",
    "        context = paragraphs[j][\"context\"]\n",
    "        qas = paragraphs[j][\"qas\"]\n",
    "        for k in range(len(qas)):\n",
    "            id_ = qas[k][\"id\"]\n",
    "            answer = qas[k][\"answer\"]\n",
    "            question = qas[k][\"question\"]\n",
    "            ids.append(id_)\n",
    "            titles[id_] = title\n",
    "            contexts.append(unicodedata.normalize('NFKD',context).encode('ascii', 'ignore'))\n",
    "            contexts_uni[id_] = context\n",
    "            answers_start[id_] = answer[\"answer_start\"]\n",
    "            answers_text[id_] = answer[\"text\"]\n",
    "            questions.append(unicodedata.normalize('NFKD',question).encode('ascii', 'ignore'))\n",
    "            questions_uni[id_] = question\n",
    "max_para = 766\n",
    "max_q = 125\n",
    "dimension = 100\n",
    "t = [i for i in range(len(ids))]\n",
    "t.sort(key=lambda x: np.random.random())\n",
    "ind = t[:61379]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_starts = np.array([create_one_hot_answer(contexts_uni[ids[ind[i]]], answers_text[ids[ind[i]]], answers_start[ids[ind[i]]], \"s\", max_para) for i in range(len(ind))])\n",
    "ans_ends = np.array([create_one_hot_answer(contexts_uni[ids[ind[i]]], answers_text[ids[ind[i]]], answers_start[ids[ind[i]]], \"e\", max_para) for i in range(len(ind))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(contexts)\n",
    "para_vocab_size = len(t.word_index) + 1\n",
    "encoded_para = t.texts_to_sequences(contexts)\n",
    "padded_para = pad_sequences(encoded_para, maxlen=max_para, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_para = zeros((para_vocab_size, dimension))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_para[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(questions)\n",
    "qns_vocab_size = len(t.word_index) + 1\n",
    "encoded_qns = t.texts_to_sequences(questions)\n",
    "padded_qns = pad_sequences(encoded_qns, maxlen=max_q, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_qns = zeros((qns_vocab_size, dimension))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_qns[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61379, 766)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_para.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed\n",
    "input_para = Input(shape=(max_para,))\n",
    "input_qns = Input(shape=(max_q,))\n",
    "em_paras = Embedding(para_vocab_size, dimension, weights=[embedding_para], input_length=max_para, trainable = False)(input_para)\n",
    "em_qns = Embedding(qns_vocab_size, dimension, weights=[embedding_qns], input_length = max_q, trainable = False)(input_qns)\n",
    "paras = GRU(100, return_sequences = True)(em_paras)\n",
    "qns = GRU(100, return_sequences = True)(em_qns)\n",
    "combine = keras.layers.concatenate([paras, qns], axis = 1)\n",
    "combine = Bidirectional(GRU(100, return_sequences=True))(combine)\n",
    "\n",
    "start = Conv1D(100, 5, padding='same', activation ='relu')(combine)\n",
    "start = Conv1D(50, 3, padding ='same', dilation_rate=2, activation='relu')(start)\n",
    "start = Conv1D(5, 3, padding ='same', dilation_rate=2, activation = 'relu')(start)\n",
    "start = Flatten()(start)\n",
    "start = Dropout(0.5)(start)\n",
    "start = Dense(766, activation='softmax', name = 'output_1')(start)\n",
    "end = GRU(200, return_sequences = True)(combine)\n",
    "end = Conv1D(100, 5, padding='same', activation ='relu')(end)\n",
    "end = Conv1D(50, 3, padding ='same', dilation_rate=2, activation='relu')(end)\n",
    "end = Conv1D(5, 3, padding ='same', dilation_rate=2, activation = 'relu')(end)\n",
    "end = Flatten()(end)\n",
    "end = Dropout(0.5)(end)\n",
    "end = Dense(766, activation='softmax', name = 'output_2')(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_para, input_qns],[start, end])\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'output_1': 'categorical_crossentropy', 'output_2': 'categorical_crossentropy'},\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 766)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 125)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)         (None, 766, 100)      6844000                                      \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 125, 100)      3168500                                      \n",
      "____________________________________________________________________________________________________\n",
      "gru_10 (GRU)                     (None, 766, 100)      60300                                        \n",
      "____________________________________________________________________________________________________\n",
      "gru_11 (GRU)                     (None, 125, 100)      60300                                        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 891, 100)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 891, 200)      120600                                       \n",
      "____________________________________________________________________________________________________\n",
      "gru_13 (GRU)                     (None, 891, 200)      240600                                       \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 891, 100)      100100                                       \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)               (None, 891, 100)      100100                                       \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 891, 50)       15050                                        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)               (None, 891, 50)       15050                                        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 891, 5)        755                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 891, 5)        755                                          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 4455)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 4455)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 4455)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 4455)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "output_1 (Dense)                 (None, 766)           3413296                                      \n",
      "____________________________________________________________________________________________________\n",
      "output_2 (Dense)                 (None, 766)           3413296                                      \n",
      "====================================================================================================\n",
      "Total params: 17,552,702.0\n",
      "Trainable params: 7,540,202.0\n",
      "Non-trainable params: 10,012,500.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49103 samples, validate on 12276 samples\n",
      "Epoch 1/10\n",
      " 2656/49103 [>.............................] - ETA: 6203s - loss: 11.0302 - output_1_loss: 5.4712 - output_2_loss: 5.5590 - output_1_acc: 0.0200 - output_2_acc: 0.0083"
     ]
    }
   ],
   "source": [
    "model.fit([padded_para, padded_qns], [ans_starts, ans_ends],\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
