{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent, Bidirectional, Dense, Flatten\n",
    "from keras.layers import Activation,recurrent, Bidirectional, Dense, Flatten, Conv1D, Lambda\n",
    "from keras.layers import Dropout, LSTM, GRU, concatenate, multiply, add, Reshape, MaxPooling1D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import keras\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.engine.topology import Layer\n",
    "from nltk.tokenize.moses import MosesDetokenizer, MosesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dictionary(vocab):\n",
    "    d = dict()\n",
    "    with open(vocab) as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            values = l.strip().split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            d[word] = coefs\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = get_dictionary(\"glove/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test.json') as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "titles = dict()\n",
    "contexts = dict()\n",
    "questions = dict()\n",
    "answers_text = dict()\n",
    "answers_start = dict()\n",
    "for i in range(len(data)):\n",
    "    paragraphs = data[i][\"paragraphs\"]\n",
    "    title = data[i][\"title\"]\n",
    "    for j in range(len(paragraphs)):\n",
    "        context = paragraphs[j][\"context\"]\n",
    "        qas = paragraphs[j][\"qas\"]\n",
    "        for k in range(len(qas)):\n",
    "#             answer = qas[k][\"answer\"]\n",
    "            id_ = qas[k][\"id\"]\n",
    "            question = qas[k][\"question\"]\n",
    "            ids.append(id_)\n",
    "            titles[id_] = title\n",
    "            contexts[id_] = context\n",
    "            questions[id_] = question\n",
    "#             answers_start[id_] = answer[\"answer_start\"]\n",
    "#             answers_text[id_] = answer[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_para = 600\n",
    "max_q = 50\n",
    "dimension = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_len = len(ids)\n",
    "# test_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "# In[167]:\n",
    "\n",
    "paras = np.zeros((test_len, max_para, dimension))\n",
    "qns = np.zeros((test_len, max_q, dimension))\n",
    "exact_match = np.zeros((test_len, max_para, 3))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# In[168]:\n",
    "\n",
    "for i in range(test_len):\n",
    "    if i % 5000 == 0:\n",
    "        print(i)\n",
    "    words = word_tokenize(contexts[ids[i]])\n",
    "    qs = word_tokenize(questions[ids[i]])\n",
    "    qs_1 = [w.lower() for w in qs]\n",
    "    qs_2 = [lmtzr.lemmatize(w) for w in qs_1]\n",
    "    for j in range(min(max_para - 1, len(words))):\n",
    "        if words[j].lower() in d:\n",
    "            paras[i][j] = d[words[j].lower()]\n",
    "        if words[j] in qs:\n",
    "            exact_match[i][j][0] = 1\n",
    "        if words[j].lower() in qs_1:\n",
    "            exact_match[i][j][1] = 1\n",
    "        if lmtzr.lemmatize(words[j].lower()) in qs_2:\n",
    "            exact_match[i][j][2] = 1\n",
    "    for j in range(min(max_q - 1, len(qs))):\n",
    "        if qs[j].lower() in d:\n",
    "            qns[i][j] = d[qs[j].lower()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, name, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.name = name\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name=self.name,\n",
    "                                      shape=(input_shape[2], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.W)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[1], self.output_dim)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "P = layers.Input(shape=(max_para,dimension), dtype='float32')\n",
    "Q = layers.Input(shape=(max_q,dimension), dtype='float32')\n",
    "P_exact_match = layers.Input(shape=(max_para,3), dtype='float32')\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "alpha_w = MyLayer(128,\"alpha_w\")\n",
    "alpha_l = MyLayer(128,\"alpha_l\")\n",
    "alpha_h = MyLayer(128,\"alpha_h\")\n",
    "alpha_u = MyLayer(128,\"alpha_u\")\n",
    "alpha_s = MyLayer(128,\"alpha_s\")\n",
    "beta = MyLayer(1,\"beta\")\n",
    "gamma_s = MyLayer(250, \"gamma_s\")\n",
    "gamma_e = MyLayer(250, \"gamma_e\")\n",
    "hidden_unit = 125\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "# Word level fusion\n",
    "aligned_P = layers.Activation(\"relu\")(alpha_w(P))\n",
    "aligned_Q = layers.Activation(\"relu\")(alpha_w(Q))\n",
    "word_fusion = layers.dot([aligned_Q, aligned_P], axes=2)\n",
    "word_fusion = Reshape((max_para * max_q,))(word_fusion)\n",
    "word_fusion = layers.Activation(\"softmax\")(word_fusion)\n",
    "word_fusion = Reshape((max_q, max_para,))(word_fusion)\n",
    "word_fusion = layers.dot([word_fusion,Q],axes=1)\n",
    "\n",
    "\n",
    "# READING\n",
    "final_P = layers.concatenate([P, P_exact_match, word_fusion], axis=2)\n",
    "final_P = Dropout(.4)(final_P)\n",
    "low_P = Bidirectional(LSTM(hidden_unit,return_sequences=True))(final_P)\n",
    "low_P = Dropout(.4)(low_P)\n",
    "high_P = Bidirectional(LSTM(hidden_unit,return_sequences=True))(low_P)\n",
    "high_P = Dropout(.4)(high_P)\n",
    "\n",
    "final_Q = Dropout(.4)(Q)\n",
    "low_Q = Bidirectional(LSTM(hidden_unit,return_sequences=True))(final_Q)\n",
    "low_Q = Dropout(.4)(low_Q)\n",
    "high_Q = Bidirectional(LSTM(hidden_unit,return_sequences=True))(low_Q)\n",
    "high_Q = Dropout(.4)(high_Q)\n",
    "\n",
    "\n",
    "# Question understanding\n",
    "U_Q = layers.concatenate([low_Q, high_Q], axis=2)\n",
    "U_Q = Bidirectional(LSTM(hidden_unit,return_sequences=True))(U_Q)\n",
    "U_Q = Dropout(.4)(U_Q)\n",
    "w = beta(U_Q)\n",
    "UQ = layers.dot([w,U_Q],axes=1)\n",
    "\n",
    "# History of words\n",
    "HP = layers.concatenate([P, low_P, high_P], axis = 2)\n",
    "HQ = layers.concatenate([Q, low_Q, high_Q], axis = 2)\n",
    "\n",
    "# Low level fusion\n",
    "low_HP = layers.Activation(\"relu\")(alpha_l(HP))\n",
    "low_HQ = layers.Activation(\"relu\")(alpha_l(HQ))\n",
    "low_fusion = layers.dot([low_HQ, low_HP], axes=2)\n",
    "low_fusion = Reshape((max_para * max_q,))(low_fusion)\n",
    "low_fusion = layers.Activation(\"softmax\")(low_fusion)\n",
    "low_fusion = Reshape((max_q, max_para,))(low_fusion)\n",
    "low_fusion = layers.dot([low_fusion,low_Q],axes=1)\n",
    "low_fusion = Dropout(.4)(low_fusion)\n",
    "\n",
    "# High level fusion\n",
    "high_HP = layers.Activation(\"relu\")(alpha_h(HP))\n",
    "high_HQ = layers.Activation(\"relu\")(alpha_h(HQ))\n",
    "high_fusion = layers.dot([high_HQ, high_HP], axes=2)\n",
    "high_fusion = Reshape((max_para * max_q,))(high_fusion)\n",
    "high_fusion = layers.Activation(\"softmax\")(high_fusion)\n",
    "high_fusion = Reshape((max_q, max_para,))(high_fusion)\n",
    "high_fusion = layers.dot([high_fusion,high_Q],axes=1)\n",
    "high_fusion = Dropout(.4)(high_fusion)\n",
    "\n",
    "# Understanding level fusion\n",
    "U_HP = layers.Activation(\"relu\")(alpha_u(HP))\n",
    "U_HQ = layers.Activation(\"relu\")(alpha_u(HQ))\n",
    "U_fusion = layers.dot([U_HQ, U_HP], axes=2)\n",
    "U_fusion = Reshape((max_para * max_q,))(U_fusion)\n",
    "U_fusion = layers.Activation(\"softmax\")(U_fusion)\n",
    "U_fusion = Reshape((max_q, max_para,))(U_fusion)\n",
    "U_fusion = layers.dot([U_fusion,U_Q],axes=1)\n",
    "U_fusion = Dropout(.4)(U_fusion)\n",
    "\n",
    "# Fully-aware attention\n",
    "P_fusion = layers.concatenate([low_P, high_P, low_fusion, high_fusion, U_fusion], axis=2)\n",
    "V_P = Bidirectional(LSTM(hidden_unit,return_sequences=True))(P_fusion)\n",
    "V_P = Dropout(.4)(V_P)\n",
    "H_P = layers.concatenate([P, P_fusion, V_P], axis=2)\n",
    "\n",
    "# Self-boosted fusion\n",
    "self_HP = layers.Activation(\"relu\")(alpha_s(H_P))\n",
    "self_fusion = layers.dot([self_HP, self_HP], axes=2)\n",
    "self_fusion = Reshape((max_para * max_para,))(self_fusion)\n",
    "self_fusion = layers.Activation(\"softmax\")(self_fusion)\n",
    "self_fusion = Reshape((max_para, max_para))(self_fusion)\n",
    "self_fusion = layers.dot([self_fusion,V_P],axes=1)\n",
    "self_fusion = Dropout(.4)(self_fusion)\n",
    "\n",
    "U_P = layers.concatenate([V_P, self_fusion], axis=2)\n",
    "U_P = Bidirectional(LSTM(hidden_unit,return_sequences=True))(U_P)\n",
    "U_P = Dropout(.4)(U_P)\n",
    "\n",
    "start = gamma_s(U_P)\n",
    "start = layers.dot([start, UQ], axes=2)\n",
    "start = Reshape((max_para,))(start)\n",
    "start = layers.Activation(\"softmax\")(start)\n",
    "\n",
    "end = gamma_e(U_P)\n",
    "end = layers.dot([end, UQ], axes=2)\n",
    "end = Reshape((max_para,))(end)\n",
    "end = layers.Activation(\"softmax\")(end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model([P, Q, P_exact_match],[start, end])\n",
    "model.load_weights('fusion.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36790/36790 [==============================] - 7099s 193ms/step\n"
     ]
    }
   ],
   "source": [
    "out = model.predict([paras,qns,exact_match], verbose=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_1 = np.argmax(out[0], axis=1)\n",
    "out_2 = np.argmax(out[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(para, pos):\n",
    "    word_list = word_tokenize(para)\n",
    "    s = ''.join(word_list[:pos])\n",
    "    if pos == 0:\n",
    "        return 0\n",
    "    if len(word_list) <= pos:\n",
    "        return len(para)\n",
    "    else:\n",
    "        begin, end = 0, len(para)\n",
    "        c = 0\n",
    "        while begin < end and c < 100:\n",
    "            mid = (begin + end) // 2\n",
    "            c += 1\n",
    "            a = word_tokenize(para[:mid])\n",
    "            s_ = ''.join(a)\n",
    "            if (len(s_) < len(s)):\n",
    "                begin = mid + 1\n",
    "            elif (len(s_) > len(s)):\n",
    "                end = mid\n",
    "            elif s == s_:\n",
    "                break\n",
    "        return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "for i in range(test_len):\n",
    "    s = 0\n",
    "    t = 1\n",
    "    if i%5000 == 0: \n",
    "        print(i)\n",
    "    l = len(word_tokenize(contexts[ids[i]]))\n",
    "    for j in range(min(l, max_para)):\n",
    "        for k in range(j, min(j+15, min(l, max_para))):\n",
    "            if out[0][i][s] * out[1][i][t] < out[0][i][j] * out[1][i][k]:\n",
    "                s = j\n",
    "                t = k\n",
    "    out_1[i] = s\n",
    "    out_2[i] = t\n",
    "#     print(s, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_2 = out_2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for i in range(5000):\n",
    "#     a = search(contexts[ids[i]], out_1[i], out_2[i])\n",
    "#     if normalize_answer(a) == normalize_answer(answers_text[ids[i]]):\n",
    "#         c += 1\n",
    "# print(c/5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"fusion.csv\", 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(['Id','Answer'])\n",
    "    for i in range(test_len):\n",
    "        a = search(contexts[ids[i]], out_1[i])\n",
    "        b = search(contexts[ids[i]], out_2[i])\n",
    "        writer.writerow([ids[i],normalize_answer(contexts[ids[i]][a:b])])\n",
    "        # writer.writerow([ids[i],normalize_answer(contexts[ids[i]][a:b]).encode('utf-8')])\n",
    "        if i%5000 == 0: \n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
